{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSUut-8S_Kvs",
        "colab_type": "code",
        "outputId": "81226b36-b203-4143-b4b4-35b0796fd4e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "!pip install -q tensorflow-gpu==2.0.0-beta0\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 348.9MB 64kB/s \n",
            "\u001b[K     |████████████████████████████████| 501kB 42.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.1MB 36.4MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtdVwIjM-3z4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  ...\n",
        "  def forward(self, queries, keys, values):\n",
        "        \n",
        "        # Do a linear for each component\n",
        "        queries = self.query_linear(queries)\n",
        "        keys = self.key_linear(keys)\n",
        "        values = self.value_linear(values)\n",
        "        \n",
        "        # Split into multiple heads\n",
        "        queries = self._split_heads(queries)\n",
        "        keys = self._split_heads(keys)\n",
        "        values = self._split_heads(values)\n",
        "        \n",
        "        # Scale queries\n",
        "        queries *= self.query_scale\n",
        "        \n",
        "        # Combine queries and keys\n",
        "        logits = torch.matmul(queries, keys.permute(0, 1, 3, 2))\n",
        "        \n",
        "        # Add bias to mask future values\n",
        "        if self.bias_mask is not None:\n",
        "            logits += Variable(self.bias_mask[:, :, :logits.shape[-2], :logits.shape[-1]].type_as(logits.data))\n",
        "        \n",
        "        # Convert to probabilites\n",
        "        weights = nn.functional.softmax(logits, dim=-1)\n",
        "        \n",
        "        # Dropout\n",
        "        weights = self.dropout(weights)\n",
        "        \n",
        "        # Combine with values to get context\n",
        "        contexts = torch.matmul(weights, values)\n",
        "        \n",
        "        # Merge heads\n",
        "        contexts = self._merge_heads(contexts)\n",
        "        \n",
        "        # Linear to get output\n",
        "        outputs = self.output_linear(contexts)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-ig-8EZqgqT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def attention(q, k, v, mask):\n",
        "  \"\"\"Calculate the attention weights.\n",
        "  q, k, v must have matching leading dimensions.\n",
        "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "  The mask has different shapes depending on its type(padding or look ahead) \n",
        "  but it must be broadcastable for addition.\n",
        "  \n",
        "  Args:\n",
        "    q: query shape == (..., seq_len_q, depth)\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    mask: Float tensor with shape broadcastable \n",
        "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "    \n",
        "  Returns:\n",
        "    output, attention_weights\n",
        "  \"\"\"\n",
        "\n",
        "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "  \n",
        "  # scale matmul_qk\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "  # add the mask to the scaled tensor.\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "  # add up to 1.\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "  return output, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca8l6XtF_i2L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads):"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}