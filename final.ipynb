{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3,
    "colab": {
      "name": "final.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nilanshrajput/Video_Generation_Transformer/blob/master/final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RJz4WUwTnmc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "from collections import OrderedDict\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "import functools\n",
        "import torchvision.models as models\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "\n",
        "from torchvision import models\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torch.autograd import Variable\n",
        "from torchvision import transforms, utils\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from skimage import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import re\n",
        "\n",
        "import argparse\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "from glob import glob\n",
        "from shutil import copyfile\n",
        "\n",
        "\n",
        "from torch import autograd\n",
        "from torch.nn import functional as F\n",
        "import torch.nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.distributed as dist\n",
        "import torch.optim\n",
        "import torch.multiprocessing as mp\n",
        "import torch.utils.data\n",
        "import torch.utils.data.distributed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaBLoPcRTnmg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import re\n",
        "import math\n",
        "# Suppose you are trying to load pre-trained resnet model in directory- models\\resnet\n",
        "\n",
        "os.environ['TORCH_HOME'] = 'D:\\dev\\Pytorch_Models\\models\\\\resnet' #setting the environment variable\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLdnAS6ETnmi",
        "colab_type": "code",
        "outputId": "cf6ed47b-e4d3-47af-e7cf-4ac6621a0730",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "!wget --header=\"Host: www.cityscapes-dataset.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3\" --header=\"Accept-Language: en-GB,en-US;q=0.9,en;q=0.8\" --header=\"Referer: https://www.cityscapes-dataset.com/downloads/\" --header=\"Cookie: PHPSESSID=4f0gk1d5hbc0ikttngvpm8ff52\" --header=\"Connection: keep-alive\" \"https://www.cityscapes-dataset.com/file-handling/?packageID=4\" -O \"leftImg8bit_trainextra.zip\" -c"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-20 10:09:30--  https://www.cityscapes-dataset.com/file-handling/?packageID=4\n",
            "Resolving www.cityscapes-dataset.com (www.cityscapes-dataset.com)... 139.19.217.8\n",
            "Connecting to www.cityscapes-dataset.com (www.cityscapes-dataset.com)|139.19.217.8|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 47230218747 (44G) [application/octet-stream]\n",
            "Saving to: ‘leftImg8bit_trainextra.zip’\n",
            "\n",
            "leftImg8bit_trainex 100%[===================>]  43.99G  69.1MB/s    in 11m 27s \n",
            "\n",
            "2019-11-20 10:21:03 (65.6 MB/s) - ‘leftImg8bit_trainextra.zip’ saved [47230218747/47230218747]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTndZBfHtTZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip leftImg8bit_trainextra.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riBpMJFiTnmj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import json\n",
        "import os\n",
        "from collections import namedtuple\n",
        "import zipfile\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "class CityscapeDataset(Dataset):\n",
        "    \"\"\"`Cityscapes <http://www.cityscapes-dataset.com/>`_ Dataset.\n",
        "\n",
        "    Args:\n",
        "        root (string): Root directory of dataset where directory ``leftImg8bit``\n",
        "            \n",
        "        split (string, optional): The image split to use, ``train``, ``train_extra`` or ``val``\n",
        "       \n",
        "        transform (callable, optional): A function/transform that takes in a PIL image\n",
        "            and returns a transformed version. E.g, ``transforms.RandomCrop``  \n",
        "        transforms (callable, optional): A function/transform that takes input sample and its target as entry\n",
        "            and returns a transformed version.\n",
        "\n",
        "    Examples:\n",
        "\n",
        "      \n",
        "    \"\"\"\n",
        "\n",
        "    # Based on https://github.com/mcordts/cityscapesScripts\n",
        "  \n",
        "\n",
        "    def __init__(self, root=None, split='train_extra', transforms=None):\n",
        "        if root is not None:\n",
        "          self.images_dir = os.path.join(self.root, 'leftImg8bit', split)\n",
        "        else:\n",
        "          self.images_dir = os.path.join('leftImg8bit', split)\n",
        "        self.split = split\n",
        "        self.images = []\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "  \n",
        "        #valid_modes = (\"train\", \"train_extra\", \"val\")\n",
        "        \n",
        "        for city in os.listdir(self.images_dir):\n",
        "            img_dir = os.path.join(self.images_dir, city)\n",
        "            images_city=[]\n",
        "            for file_name in os.listdir(img_dir):\n",
        "                \n",
        "                images_city.append(os.path.join(img_dir, file_name))\n",
        "            # re.split(\\d+,input) splits by integer value\n",
        "            images_city.sort( key= lambda text: int(re.split('(\\d+)',text)[3]+re.split('(\\d+)',text) [5] ))\n",
        "            self.images+=images_city\n",
        "        \n",
        "    def print_dir(self):\n",
        "      print(self.images)\n",
        "\n",
        "    def im_path(self):\n",
        "      return self.images            \n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "        Returns:\n",
        "             image \n",
        "        \"\"\"\n",
        "\n",
        "        image = Image.open(self.images[index]).convert('RGB')\n",
        "        if self.transforms is not None:\n",
        "            image = self.transforms(image)\n",
        "\n",
        "        return image\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6K9KmFu2Tnml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import json\n",
        "import os\n",
        "from collections import namedtuple\n",
        "import zipfile\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "class CityscapeDataset(Dataset):\n",
        "    \"\"\"`Cityscapes <http://www.cityscapes-dataset.com/>`_ Dataset.\n",
        "\n",
        "    Args:\n",
        "        root (string): Root directory of dataset where directory ``leftImg8bit``\n",
        "            \n",
        "        split (string, optional): The image split to use, ``train``, ``train_extra`` or ``val``\n",
        "       \n",
        "        transform (callable, optional): A function/transform that takes in a PIL image\n",
        "            and returns a transformed version. E.g, ``transforms.RandomCrop``  \n",
        "        transforms (callable, optional): A function/transform that takes input sample and its target as entry\n",
        "            and returns a transformed version.\n",
        "\n",
        "    Examples:\n",
        "\n",
        "      \n",
        "    \"\"\"\n",
        "\n",
        "    # Based on https://github.com/mcordts/cityscapesScripts\n",
        "  \n",
        "\n",
        "    def __init__(self, root=None, split='train_extra', transforms=None):\n",
        "        if root is not None:\n",
        "          self.images_dir = os.path.join(self.root, 'leftImg8bit', split)\n",
        "        else:\n",
        "          self.images_dir = os.path.join('leftImg8bit', split)\n",
        "        self.split = split\n",
        "        self.images = []\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "  \n",
        "        #valid_modes = (\"train\", \"train_extra\", \"val\")\n",
        "        \n",
        "        for city in os.listdir(self.images_dir):\n",
        "            img_dir = os.path.join(self.images_dir, city)\n",
        "            images_city=[]\n",
        "            for file_name in os.listdir(img_dir):\n",
        "                \n",
        "                images_city.append(os.path.join(img_dir, file_name))\n",
        "            # re.split(\\d+,input) splits by integer value\n",
        "            images_city.sort( key= lambda text: int(re.split('(\\d+)',text)[3]+re.split('(\\d+)',text) [5] ))\n",
        "            self.images+=images_city\n",
        "        \n",
        "    def print_dir(self):\n",
        "      print(self.images)\n",
        "\n",
        "    def im_path(self):\n",
        "      return self.images            \n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "        Returns:\n",
        "             image \n",
        "        \"\"\"\n",
        "\n",
        "        image = Image.open(self.images[index]).convert('RGB')\n",
        "        if self.transforms is not None:\n",
        "            image = self.transforms(image)\n",
        "\n",
        "        return image\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LdC3zJtTnmm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaler = transforms.Resize((224, 224))\n",
        "to_tensor = transforms.ToTensor()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpSu6DcVTnmo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#not adding normalization as image looses textures and clrs for recostruction\n",
        "\n",
        "composed_transforms=transforms.Compose([scaler,to_tensor])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "598CAaijTnmq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transformed_dataset = CityscapeDataset(\n",
        "                                           transforms=composed_transforms\n",
        "                                           )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RV0CGV5kTnmr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvSg9ccTTnmt",
        "colab_type": "code",
        "outputId": "e22d28f2-9d25-49c5-f791-8e174c1d1994",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        }
      },
      "source": [
        "x=next(iter(dataloader))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-e9df2bd41fb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'dataloader' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iJugmqRTnmu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResNet50Bottom(nn.Module):\n",
        "    def __init__(self, original_model):\n",
        "        super(ResNet50Bottom, self).__init__()\n",
        "        self.features = nn.Sequential(*list(original_model.children())[:-5])\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5P4hTd5fTnmw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#res50_model = models.resnet50(pretrained=True)\n",
        "#res50_conv2 = ResNet50Bottom(res50_model).cuda()\n",
        "#\n",
        "#outputs = res50_conv2(x.to('cuda'))\n",
        "#outputs.data.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBMR6P1dDsQh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FeatureChanger(nn.Module):\n",
        "    def __init__(self,in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.kernel_size = 1\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.conv2d_0 = torch.nn.Conv2d(in_channels, out_channels, self.kernel_size, stride=1)\n",
        "        #self.con2d_1 = torch.nn.Conv2d(out_channels*2, out_channels, kernel_size, stride=1)\n",
        "        \n",
        "    def forward(self, batch):\n",
        "        x = self.conv2d_0(batch)\n",
        "        #x = self.con2d_1(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4cE65evRDKb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MaskedSelfAttention(nn.Module):\n",
        "\n",
        "  def __init__(self,heads, d_model, dropout= .4):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.heads = heads\n",
        "    self.soft2d = nn.Softmax2d()\n",
        "    self.drop = nn.Dropout(dropout)\n",
        "    \n",
        "\n",
        "  def _generate_square_subsequent_mask(self, sz):\n",
        "    #sz= batch size\n",
        "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "  def mult_4d(self, batch, q_w):\n",
        "    bs= batch.shape[0]\n",
        "    layers = batch.shape[1]\n",
        "    dim1 = batch.shape[2]\n",
        "    dim2 = batch.shape[3]\n",
        "    output = torch.bmm(batch.view(bs*layers,dim1,dim2), q_w.view(bs*layers,dim1,dim2))\n",
        "    return output.view(bs,layers,dim1,dim2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, q , k, v):\n",
        "\n",
        "    #score\n",
        "    attn_output_weights = self.mult_4d(q, k.transpose(-2,-1))/math.sqrt(self.d_model/2)\n",
        "    soft_scores = self.soft2d(attn_output_weights)\n",
        "    soft_scores = self.drop(soft_scores).to('cuda')\n",
        "    output = self.mult_4d(v, soft_scores)\n",
        "\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-iPL_1-Tnmx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, bs, heads, d_model, dim1, dim2, dropout = 0.4):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.h = heads\n",
        "        self.dim1 = dim1\n",
        "        self.dim2 =dim2\n",
        "        self.dropout = dropout\n",
        "        self.nu_feat =d_model # int(d_model/self.h*2)\n",
        "        self.bs = bs\n",
        "        res50_model = models.resnet50(pretrained=True)\n",
        "        self.resnet_back = ResNet50Bottom(res50_model).cuda()\n",
        "\n",
        "\n",
        "        self.q_3d = torch.nn.Parameter(torch.empty(bs,self.nu_feat,dim1,dim2,requires_grad=True))\n",
        "        self.k_3d = torch.nn.Parameter(torch.empty(bs,self.nu_feat,dim1,dim2,requires_grad=True))\n",
        "        self.v_3d = torch.nn.Parameter(torch.empty(bs,self.nu_feat,dim1,dim2,requires_grad=True))\n",
        "   \n",
        "\n",
        "        nn.init.kaiming_normal_(self.q_3d, mode='fan_in')\n",
        "        nn.init.kaiming_normal_(self.k_3d, mode='fan_in')\n",
        "        nn.init.kaiming_normal_(self.v_3d, mode='fan_in')\n",
        "\n",
        "        self.attention_layer= MaskedSelfAttention(self.h,d_model,dropout)\n",
        "    \n",
        "    def mult_4d(self, batch, q_w, bs):\n",
        "      \n",
        "      layers = batch.shape[1]\n",
        "      dim1 = batch.shape[2]\n",
        "      dim2 = batch.shape[3]\n",
        "      output = torch.bmm(batch.view(bs*layers,dim1,dim2), q_w.view(bs*layers,dim1,dim2))\n",
        "      return output.view(bs,layers,dim1,dim2)\n",
        "\n",
        "    def forward(self, batch, mask=None):\n",
        "        bs = batch.size(0)\n",
        "        batch = batch.to('cuda')\n",
        "        \"\"\" \n",
        "        batch = self.resnet_back(batch.to('cuda'))        \n",
        "        channels = batch.shape[1]\n",
        "        feature_layer1 = FeatureChanger(channels,int(channels/self.h*2)).cuda()\n",
        "        batch = feature_layer1(batch)\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        q = self.mult_4d(batch,self.q_3d.to('cuda'),bs)\n",
        "        k = self.mult_4d(batch,self.k_3d.to('cuda'),bs)\n",
        "        v = self.mult_4d(batch,self.v_3d.to('cuda'),bs)\n",
        "\n",
        "        \"\"\"\n",
        "        #divide feature space in no of heads each head works on certain features only\n",
        "        #returns tuple of size=heads\n",
        "\n",
        "        batch_chunks= torch.chunk(batch, heads, dim=1)\n",
        "        #converting tuple to tensor\n",
        "        batch_chunks = torch.stack(batch_chunks, dim =0)\n",
        "        \"\"\"\n",
        "\n",
        "        attention_out = self.attention_layer( q, k, v)\n",
        "        #feature_layer2 = FeatureChanger(int(channels/self.h*2),channels).cuda()\n",
        "        #output = feature_layer2(batch)\n",
        "        output = attention_out\n",
        "  \n",
        "    \n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wiy5LxisgFu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 2\n",
        "dataloader = DataLoader(transformed_dataset, batch_size,\n",
        "                        sampler=torch.utils.data.SequentialSampler(transformed_dataset) , num_workers=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFa4dkniuO4D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "heads = 8\n",
        "d_model = 3\n",
        "dim1 = dim2= 224\n",
        "dropout = 0.4\n",
        "\n",
        "\n",
        "MHA= MultiHeadAttention(batch_size, heads, d_model, dim1, dim2, dropout = 0.4)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsgWDhCXy_uQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "output = MHA(next(iter(dataloader)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BsfTOGr_Tk7",
        "colab_type": "code",
        "outputId": "dcf145db-01b9-4e64-9ae5-70d41c600435",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "output.shape"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3, 224, 224])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4H_WjWwsg9Ei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# taken from singan implementaion from repo given below\n",
        "# https://github.com/FriedRonaldo/SinGAN\n",
        "#\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, img_size_min, num_scale, bs, heads, d_model, dim1, dim2, dropout = 0.4, scale_factor=4/3):\n",
        "        super(Generator, self).__init__()\n",
        "        self.img_size_min = img_size_min\n",
        "        self.scale_factor = scale_factor\n",
        "        self.num_scale = num_scale\n",
        "        self.nf = 32\n",
        "        self.current_scale = 0\n",
        "        self.d_model = d_model\n",
        "        self.h = heads\n",
        "        self.dim1 = dim1\n",
        "        self.dim2 =dim2\n",
        "        self.dropout = dropout\n",
        "        self.nu_feat =d_model # int(d_model/self.h*2)\n",
        "        self.bs = bs\n",
        "\n",
        "        self.size_list = [int(self.img_size_min * scale_factor**i) for i in range(num_scale + 1)]\n",
        "        print(self.size_list)\n",
        "        self.MHA= MultiHeadAttention(bs, heads, d_model, dim1, dim2, dropout )\n",
        "        self.sub_generators = nn.ModuleList()\n",
        "\n",
        "        first_generator = nn.ModuleList()\n",
        "\n",
        "        first_generator.append(nn.Sequential(nn.Conv2d(3, self.nf, 3, 1),\n",
        "                                             nn.BatchNorm2d(self.nf),\n",
        "                                             nn.LeakyReLU(2e-1)))\n",
        "        for _ in range(3):\n",
        "            first_generator.append(nn.Sequential(nn.Conv2d(self.nf, self.nf, 3, 1),\n",
        "                                                 nn.BatchNorm2d(self.nf),\n",
        "                                                 nn.LeakyReLU(2e-1)))\n",
        "\n",
        "        first_generator.append(nn.Sequential(nn.Conv2d(self.nf, 3, 3, 1),\n",
        "                                             nn.Tanh()))\n",
        "\n",
        "        first_generator = nn.Sequential(*first_generator)\n",
        "\n",
        "        self.sub_generators.append(first_generator)\n",
        "\n",
        "    def forward(self, batch, img=None):\n",
        "        x_list = []\n",
        "        x_first = self.sub_generators[0](batch)\n",
        "        x_list.append(x_first)\n",
        "\n",
        "        if img is not None:\n",
        "            x_inter = img\n",
        "        else:\n",
        "            x_inter = x_first\n",
        "\n",
        "        for i in range(1, self.current_scale + 1):\n",
        "            x_inter = F.interpolate(x_inter, (self.size_list[i], self.size_list[i]), mode='bilinear', align_corners=True)\n",
        "            x_prev = x_inter\n",
        "            x_inter = F.pad(x_inter, [5, 5, 5, 5], value=0)\n",
        "            x_inter = x_inter + z[i]\n",
        "            x_inter = self.sub_generators[i](x_inter) + x_prev\n",
        "            x_list.append(x_inter)\n",
        "\n",
        "        return x_list\n",
        "\n",
        "    def progress(self):\n",
        "        self.current_scale += 1\n",
        "\n",
        "        if self.current_scale % 4 == 0:\n",
        "            self.nf *= 2\n",
        "\n",
        "        tmp_generator = nn.ModuleList()\n",
        "        tmp_generator.append(nn.Sequential(nn.Conv2d(3, self.nf, 3, 1),\n",
        "                                           nn.BatchNorm2d(self.nf),\n",
        "                                           nn.LeakyReLU(2e-1)))\n",
        "\n",
        "        for _ in range(3):\n",
        "            tmp_generator.append(nn.Sequential(nn.Conv2d(self.nf, self.nf, 3, 1),\n",
        "                                               nn.BatchNorm2d(self.nf),\n",
        "                                               nn.LeakyReLU(2e-1)))\n",
        "\n",
        "        tmp_generator.append(nn.Sequential(nn.Conv2d(self.nf, 3, 3, 1),\n",
        "                                           nn.Tanh()))\n",
        "\n",
        "        tmp_generator = nn.Sequential(*tmp_generator)\n",
        "\n",
        "        if self.current_scale % 4 != 0:\n",
        "            prev_generator = self.sub_generators[-1]\n",
        "\n",
        "            # Initialize layers via copy\n",
        "            if self.current_scale >= 1:\n",
        "                tmp_generator.load_state_dict(prev_generator.state_dict())\n",
        "\n",
        "        self.sub_generators.append(tmp_generator)\n",
        "        print(\"GENERATOR PROGRESSION DONE\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RKtRVMPg2G2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# taken from singan implementaion from repo given below\n",
        "# https://github.com/FriedRonaldo/SinGAN\n",
        "#\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.nf = 32\n",
        "        self.current_scale = 0\n",
        "\n",
        "        self.sub_discriminators = nn.ModuleList()\n",
        "\n",
        "        first_discriminator = nn.ModuleList()\n",
        "\n",
        "        first_discriminator.append(nn.Sequential(nn.Conv2d(3, self.nf, 3, 1, 1),\n",
        "                                             nn.LeakyReLU(2e-1)))\n",
        "        for _ in range(3):\n",
        "            first_discriminator.append(nn.Sequential(nn.Conv2d(self.nf, self.nf, 3, 1, 1),\n",
        "                                                 nn.BatchNorm2d(self.nf),\n",
        "                                                 nn.LeakyReLU(2e-1)))\n",
        "\n",
        "        first_discriminator.append(nn.Sequential(nn.Conv2d(self.nf, 1, 3, 1, 1)))\n",
        "\n",
        "        first_discriminator = nn.Sequential(*first_discriminator)\n",
        "\n",
        "        self.sub_discriminators.append(first_discriminator)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.sub_discriminators[self.current_scale](x)\n",
        "        return out\n",
        "\n",
        "    def progress(self):\n",
        "        self.current_scale += 1\n",
        "        # Lower scale discriminators are not used in later ... replace append to assign?\n",
        "        if self.current_scale % 4 == 0:\n",
        "            self.nf *= 2\n",
        "\n",
        "        tmp_discriminator = nn.ModuleList()\n",
        "        tmp_discriminator.append(nn.Sequential(nn.Conv2d(3, self.nf, 3, 1, 1),\n",
        "                                               nn.LeakyReLU(2e-1)))\n",
        "\n",
        "        for _ in range(3):\n",
        "            tmp_discriminator.append(nn.Sequential(nn.Conv2d(self.nf, self.nf, 3, 1, 1),\n",
        "                                                   nn.BatchNorm2d(self.nf),\n",
        "                                                   nn.LeakyReLU(2e-1)))\n",
        "\n",
        "        tmp_discriminator.append(nn.Sequential(nn.Conv2d(self.nf, 1, 3, 1, 1)))\n",
        "\n",
        "        tmp_discriminator = nn.Sequential(*tmp_discriminator)\n",
        "\n",
        "        if self.current_scale % 4 != 0:\n",
        "            prev_discriminator = self.sub_discriminators[-1]\n",
        "\n",
        "            # Initialize layers via copy\n",
        "            if self.current_scale >= 1:\n",
        "                tmp_discriminator.load_state_dict(prev_discriminator.state_dict())\n",
        "\n",
        "        self.sub_discriminators.append(tmp_discriminator)\n",
        "        print(\"DISCRIMINATOR PROGRESSION DONE\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfjLSAcE-EMc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiX6cKOcg1_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# taken from singan implementaion from repo given below\n",
        "# https://github.com/FriedRonaldo/SinGAN\n",
        "#\n",
        "\n",
        "from tqdm import trange\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "\n",
        "def trainSinGAN(data_loader, networks, opts, stage, args, additional):\n",
        "  # avg meter\n",
        "  d_losses = AverageMeter()\n",
        "  g_losses = AverageMeter()\n",
        "\n",
        "  # set nets\n",
        "  D = networks[0]\n",
        "  G = networks[1]\n",
        "  # set opts\n",
        "  d_opt = opts['d_opt']\n",
        "  g_opt = opts['g_opt']\n",
        "  # switch to train mode\n",
        "  D.train()\n",
        "  G.train()\n",
        "  # summary writer\n",
        "  # writer = additional[0]\n",
        "  train_it = iter(data_loader)\n",
        "  # total_iter = 2000 * (args.num_scale - stage + 1)\n",
        "  # decay_lr = 1600 * (args.num_scale - stage + 1)\n",
        "  total_iter = 2000\n",
        "  decay_lr = 1600\n",
        "\n",
        "  d_iter = 3\n",
        "  g_iter = 3\n",
        "\n",
        "  t_train = trange(0, total_iter, initial=0, total=total_iter)\n",
        "  x_in = next(train_it)\n",
        "\n",
        "  x_in = x_in.cuda( non_blocking=True)\n",
        "  x_org = x_in\n",
        "  x_in = F.interpolate(x_in, (args.size_list[stage], args.size_list[stage]), mode='bilinear', align_corners=True)\n",
        "\n",
        "  \"\"\"\n",
        "  z_rec = additional['z_rec']\n",
        "\n",
        "  for z_idx in range(len(z_rec)):\n",
        "      z_rec[z_idx] = z_rec[z_idx].cuda(args.gpu, non_blocking=True)\n",
        "\n",
        "  x_in = next(train_it)\n",
        "\n",
        "  x_in = x_in.cuda(args.gpu, non_blocking=True)\n",
        "  x_org = x_in\n",
        "  x_in = F.interpolate(x_in, (args.size_list[stage], args.size_list[stage]), mode='bilinear', align_corners=True)\n",
        "  vutils.save_image(x_in.detach().cpu(), os.path.join(args.res_dir, 'ORGTRAIN_{}.png'.format(stage)),\n",
        "                    nrow=1, normalize=True)\n",
        "\n",
        "  x_in_list = [x_in]\n",
        "  for xidx in range(1, stage + 1):\n",
        "      x_tmp = F.interpolate(x_org, (args.size_list[xidx], args.size_list[xidx]), mode='bilinear', align_corners=True)\n",
        "      x_in_list.append(x_tmp)\n",
        "    \"\"\"\n",
        "  z_rec = x_in\n",
        "  for i in t_train:\n",
        "      if i == decay_lr:\n",
        "          for param_group in d_opt.param_groups:\n",
        "                  param_group['lr'] *= 0.1\n",
        "                  print(\"DISCRIMINATOR LEARNING RATE UPDATE TO :\", param_group['lr'])\n",
        "          for param_group in g_opt.param_groups:\n",
        "                  param_group['lr'] *= 0.1\n",
        "                  print(\"GENERATOR LEARNING RATE UPDATE TO :\", param_group['lr'])\n",
        "\n",
        "      for _ in range(g_iter):\n",
        "          g_opt.zero_grad()\n",
        "\n",
        "          x_rec_list = G(z_rec)\n",
        "          print(x_rec_list[-1].shape)\n",
        "          print(x_in.shape)\n",
        "          g_rec = F.mse_loss(x_rec_list[-1], x_in)\n",
        "          # calculate rmse for each scale\n",
        "          rmse_list = [1.0]\n",
        "          for rmseidx in range(1, stage + 1):\n",
        "              rmse = torch.sqrt(F.mse_loss(x_rec_list[rmseidx], x_in_list[rmseidx]))\n",
        "              rmse_list.append(rmse)\n",
        "\n",
        "          z_list = [F.pad(rmse_list[z_idx] * torch.randn(args.batch_size, 3, args.size_list[z_idx],\n",
        "                                              args.size_list[z_idx]).cuda(args.gpu, non_blocking=True),\n",
        "                          [5, 5, 5, 5], value=0) for z_idx in range(stage + 1)]\n",
        "\n",
        "          x_fake_list = G(z_list)\n",
        "\n",
        "          g_fake_logit = D(x_fake_list[-1])\n",
        "\n",
        "          ones = torch.ones_like(g_fake_logit).cuda(args.gpu)\n",
        "\n",
        "          if args.gantype == 'wgangp':\n",
        "              # wgan gp\n",
        "              g_fake = -torch.mean(g_fake_logit, (2, 3))\n",
        "              g_loss = g_fake + 10.0 * g_rec\n",
        "          elif args.gantype == 'zerogp':\n",
        "              # zero centered GP\n",
        "              g_fake = F.binary_cross_entropy_with_logits(g_fake_logit, ones, reduction='none').mean()\n",
        "              g_loss = g_fake + 100.0 * g_rec\n",
        "\n",
        "          elif args.gantype == 'lsgan':\n",
        "              # lsgan\n",
        "              g_fake = F.mse_loss(torch.mean(g_fake_logit, (2, 3)), 0.9 * ones)\n",
        "              g_loss = g_fake + 50.0 * g_rec\n",
        "\n",
        "          g_loss.backward()\n",
        "          g_opt.step()\n",
        "\n",
        "          g_losses.update(g_loss.item(), x_in.size(0))\n",
        "\n",
        "      # Update discriminator\n",
        "      for _ in range(d_iter):\n",
        "          x_in.requires_grad = True\n",
        "\n",
        "          d_opt.zero_grad()\n",
        "          x_fake_list = G(z_list)\n",
        "\n",
        "          d_fake_logit = D(x_fake_list[-1].detach())\n",
        "          d_real_logit = D(x_in)\n",
        "\n",
        "          ones = torch.ones_like(d_real_logit).cuda(args.gpu)\n",
        "          zeros = torch.zeros_like(d_fake_logit).cuda(args.gpu)\n",
        "\n",
        "          if args.gantype == 'wgangp':\n",
        "              # wgan gp\n",
        "              d_fake = torch.mean(d_fake_logit, (2, 3))\n",
        "              d_real = -torch.mean(d_real_logit, (2, 3))\n",
        "              d_gp = compute_grad_gp_wgan(D, x_in, x_fake_list[-1], args.gpu)\n",
        "              d_loss = d_real + d_fake + 0.1 * d_gp\n",
        "          elif args.gantype == 'zerogp':\n",
        "              # zero centered GP\n",
        "              # d_fake = F.binary_cross_entropy_with_logits(torch.mean(d_fake_logit, (2, 3)), zeros)\n",
        "              d_fake = F.binary_cross_entropy_with_logits(d_fake_logit, zeros, reduction='none').mean()\n",
        "              # d_real = F.binary_cross_entropy_with_logits(torch.mean(d_real_logit, (2, 3)), ones)\n",
        "              d_real = F.binary_cross_entropy_with_logits(d_real_logit, ones, reduction='none').mean()\n",
        "              d_gp = compute_grad_gp(torch.mean(d_real_logit, (2, 3)), x_in)\n",
        "              d_loss = d_real + d_fake + 10.0 * d_gp\n",
        "\n",
        "          elif args.gantype == 'lsgan':\n",
        "              # lsgan\n",
        "              d_fake = F.mse_loss(torch.mean(d_fake_logit, (2, 3)), zeros)\n",
        "              d_real = F.mse_loss(torch.mean(d_real_logit, (2, 3)), 0.9 * ones)\n",
        "              d_loss = d_real + d_fake\n",
        "\n",
        "          d_loss.backward()\n",
        "          d_opt.step()\n",
        "\n",
        "          d_losses.update(d_loss.item(), x_in.size(0))\n",
        "\n",
        "      t_train.set_description('Stage: [{}/{}] Avg Loss: D[{d_losses.avg:.3f}] G[{g_losses.avg:.3f}] RMSE[{rmse:.3f}]'\n",
        "                              .format(stage, args.num_scale, d_losses=d_losses, g_losses=g_losses, rmse=rmse_list[-1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKgjMKYeh7yN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Arguments():\n",
        "  def __init__(self,gpu = None,gantype = 'zerogp',model_name = 'SinGan', batch_size = 128, img_size_max= 224, img_size_min=20,\n",
        "               load_model = None, multiprocessing_distributed= None, world_size=1, log_dir= None, res_dir = None, num_scale = None):\n",
        "    self.gpu = gpu\n",
        "    self.gantype = gantype\n",
        "    self.model_name = \"SinGan\"\n",
        "    self.batch_size = batch_size\n",
        "    self.img_size_max= img_size_max\n",
        "    self.img_size_min=img_size_min\n",
        "    self.load_model = load_model\n",
        "    self.multiprocessing_distributed= multiprocessing_distributed \n",
        "    self.world_size=world_size\n",
        "    self.log_dir= log_dir\n",
        "    self.res_dir = res_dir \n",
        "    self.num_scale = num_scale \n",
        "    self.stage = None\n",
        "    self.distributed = None\n",
        "    self.workers = 8\n",
        "    self.d_model = None\n",
        "    self.h = None\n",
        "    self.dim1 = None\n",
        "    self.dim2 = None\n",
        "    self.dropout = None\n",
        "    self.nu_feat =d_model\n",
        "    self.dataset = None\n",
        "    \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_RVMJm6mlbE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def makedirs(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "def formatted_print(notice, value):\n",
        "    print('{0:<40}{1:<40}'.format(notice, value))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KE0vk8BJvF0k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def main_worker(gpu, ngpus_per_node, args):\n",
        "    args.gpu = 0\n",
        "\n",
        "\n",
        "    if args.gpu is not None:\n",
        "        print(\"Use GPU: {} for training\".format(args.gpu))\n",
        "    args.distributed = False\n",
        "    if args.distributed:\n",
        "        if args.multiprocessing_distributed:\n",
        "            args.rank = args.rank * ngpus_per_node + gpu\n",
        "        dist.init_process_group(backend='nccl', init_method='tcp://127.0.0.1:'+args.port,\n",
        "                                world_size=args.world_size, rank=args.rank)\n",
        "\n",
        "    ################\n",
        "    # Define model #\n",
        "    ################\n",
        "    # 4/3 : scale factor in the paper\n",
        "    scale_factor = 4/3\n",
        "\n",
        "\n",
        "\n",
        "    tmp_scale = args.img_size_max / args.img_size_min\n",
        "    args.num_scale = int(np.round(np.log(tmp_scale) / np.log(scale_factor)))\n",
        "    args.size_list = [int(args.img_size_min * scale_factor**i) for i in range(args.num_scale + 1)]\n",
        "\n",
        "    discriminator = Discriminator()\n",
        "    generator = Generator(args.img_size_min, args.num_scale,args.batch_size, args.heads, args.d_model, args.dim1, args.dim2, args.dropout,  scale_factor)\n",
        "\n",
        "    networks = [discriminator, generator]\n",
        "\n",
        "    if args.distributed:\n",
        "        if args.gpu is not None:\n",
        "            print('Distributed to', args.gpu)\n",
        "            torch.cuda.set_device(args.gpu)\n",
        "            networks = [x.cuda(args.gpu) for x in networks]\n",
        "            args.batch_size = int(args.batch_size / ngpus_per_node)\n",
        "            args.workers = int(args.workers / ngpus_per_node)\n",
        "            networks = [torch.nn.parallel.DistributedDataParallel(x, device_ids=[args.gpu], output_device=args.gpu) for x in networks]\n",
        "        else:\n",
        "            networks = [x.cuda() for x in networks]\n",
        "            networks = [torch.nn.parallel.DistributedDataParallel(x) for x in networks]\n",
        "\n",
        "    elif args.gpu is not None:\n",
        "        torch.cuda.set_device(args.gpu)\n",
        "        networks = [x.cuda(args.gpu) for x in networks]\n",
        "    else:\n",
        "        networks = [torch.nn.DataParallel(x).cuda() for x in networks]\n",
        "\n",
        "    discriminator, generator, = networks\n",
        "\n",
        "    ######################\n",
        "    # Loss and Optimizer #\n",
        "    ######################\n",
        "    if args.distributed:\n",
        "        d_opt = torch.optim.Adam(discriminator.module.sub_discriminators[0].parameters(), 5e-4, (0.5, 0.999))\n",
        "        g_opt = torch.optim.Adam(generator.module.sub_generators[0].parameters(), 5e-4, (0.5, 0.999))\n",
        "    else:\n",
        "        d_opt = torch.optim.Adam(discriminator.sub_discriminators[0].parameters(), 5e-4, (0.5, 0.999))\n",
        "        g_opt = torch.optim.Adam(generator.sub_generators[0].parameters(), 5e-4, (0.5, 0.999))\n",
        "\n",
        "    ##############\n",
        "    # Load model #\n",
        "    ##############\n",
        "    args.stage = 0\n",
        "    if args.load_model is not None:\n",
        "        check_load = open(os.path.join(args.log_dir, \"checkpoint.txt\"), 'r')\n",
        "        to_restore = check_load.readlines()[-1].strip()\n",
        "        load_file = os.path.join(args.log_dir, to_restore)\n",
        "        if os.path.isfile(load_file):\n",
        "            print(\"=> loading checkpoint '{}'\".format(load_file))\n",
        "            checkpoint = torch.load(load_file, map_location='cpu')\n",
        "            for _ in range(int(checkpoint['stage'])):\n",
        "                generator.progress()\n",
        "                discriminator.progress()\n",
        "            networks = [discriminator, generator]\n",
        "            if args.distributed:\n",
        "                if args.gpu is not None:\n",
        "                    print('Distributed to', args.gpu)\n",
        "                    torch.cuda.set_device(args.gpu)\n",
        "                    networks = [x.cuda(args.gpu) for x in networks]\n",
        "                    args.batch_size = int(args.batch_size / ngpus_per_node)\n",
        "                    args.workers = int(args.workers / ngpus_per_node)\n",
        "                    networks = [\n",
        "                        torch.nn.parallel.DistributedDataParallel(x, device_ids=[args.gpu], output_device=args.gpu) for\n",
        "                        x in networks]\n",
        "                else:\n",
        "                    networks = [x.cuda() for x in networks]\n",
        "                    networks = [torch.nn.parallel.DistributedDataParallel(x) for x in networks]\n",
        "\n",
        "            elif args.gpu is not None:\n",
        "                torch.cuda.set_device(args.gpu)\n",
        "                networks = [x.cuda(args.gpu) for x in networks]\n",
        "            else:\n",
        "                networks = [torch.nn.DataParallel(x).cuda() for x in networks]\n",
        "\n",
        "            discriminator, generator, = networks\n",
        "\n",
        "            args.stage = checkpoint['stage']\n",
        "            args.img_to_use = checkpoint['img_to_use']\n",
        "            discriminator.load_state_dict(checkpoint['D_state_dict'])\n",
        "            generator.load_state_dict(checkpoint['G_state_dict'])\n",
        "            d_opt.load_state_dict(checkpoint['d_optimizer'])\n",
        "            g_opt.load_state_dict(checkpoint['g_optimizer'])\n",
        "            print(\"=> loaded checkpoint '{}' (stage {})\"\n",
        "                  .format(load_file, checkpoint['stage']))\n",
        "        else:\n",
        "            print(\"=> no checkpoint found at '{}'\".format(args.log_dir))\n",
        "\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "    ###########\n",
        "    # Dataset #\n",
        "    ###########\n",
        "    transformed_dataset = CityscapeDataset(\n",
        "                                           transforms=composed_transforms\n",
        "                                           )\n",
        "\n",
        "    if args.distributed:\n",
        "        train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n",
        "    else:\n",
        "        train_sampler = None\n",
        "\n",
        "    train_loader = DataLoader(transformed_dataset, batch_size = args.batch_size,\n",
        "                            sampler=torch.utils.data.SequentialSampler(transformed_dataset) ,  num_workers=args.workers)\n",
        "  \n",
        "\n",
        "    ######################\n",
        "    # Validate and Train #\n",
        "    ######################\n",
        "    z_fix_list = [F.pad(torch.randn(args.batch_size, 3, args.size_list[0], args.size_list[0]), [5, 5, 5, 5], value=0)]\n",
        "    zero_list = [F.pad(torch.zeros(args.batch_size, 3, args.size_list[zeros_idx], args.size_list[zeros_idx]),\n",
        "                       [5, 5, 5, 5], value=0) for zeros_idx in range(1, args.num_scale + 1)]\n",
        "    z_fix_list = z_fix_list + zero_list\n",
        "    \"\"\"\n",
        "    if args.validation:\n",
        "        validateSinGAN(train_loader, networks, args.stage, args, {\"z_rec\": z_fix_list})\n",
        "        return\n",
        "\n",
        "    elif args.test:\n",
        "        validateSinGAN(train_loader, networks, args.stage, args, {\"z_rec\": z_fix_list})\n",
        "        return\n",
        "    \"\"\"\n",
        "\n",
        "    if not args.multiprocessing_distributed or (args.multiprocessing_distributed and args.rank % ngpus_per_node == 0):\n",
        "        check_list = open(os.path.join(args.log_dir, \"checkpoint.txt\"), \"a+\")\n",
        "        record_txt = open(os.path.join(args.log_dir, \"record.txt\"), \"a+\")\n",
        "        record_txt.write('DATASET\\t:\\t{}\\n'.format(args.dataset))\n",
        "        record_txt.write('GANTYPE\\t:\\t{}\\n'.format(args.gantype))\n",
        "#        record_txt.write('IMGTOUSE\\t:\\t{}\\n'.format(args.img_to_use))\n",
        "        record_txt.close()\n",
        "\n",
        "    for stage in range(args.stage, args.num_scale + 1):\n",
        "        if args.distributed:\n",
        "            train_sampler.set_epoch(stage)\n",
        "\n",
        "        trainSinGAN(train_loader, networks, {\"d_opt\": d_opt, \"g_opt\": g_opt}, stage, args, {\"z_rec\": z_fix_list})\n",
        "       # validateSinGAN(train_loader, networks, stage, args, {\"z_rec\": z_fix_list})\n",
        "\n",
        "        if args.distributed:\n",
        "            discriminator.module.progress()\n",
        "            generator.module.progress()\n",
        "        else:\n",
        "            discriminator.progress()\n",
        "            generator.progress()\n",
        "\n",
        "        networks = [discriminator, generator]\n",
        "\n",
        "        if args.distributed:\n",
        "            if args.gpu is not None:\n",
        "                print('Distributed', args.gpu)\n",
        "                torch.cuda.set_device(args.gpu)\n",
        "                networks = [x.cuda(args.gpu) for x in networks]\n",
        "                args.batch_size = int(args.batch_size / ngpus_per_node)\n",
        "                args.workers = int(args.workers / ngpus_per_node)\n",
        "                networks = [torch.nn.parallel.DistributedDataParallel(x, device_ids=[args.gpu], output_device=args.gpu)\n",
        "                            for x in networks]\n",
        "            else:\n",
        "                networks = [x.cuda() for x in networks]\n",
        "                networks = [torch.nn.parallel.DistributedDataParallel(x) for x in networks]\n",
        "\n",
        "        elif args.gpu is not None:\n",
        "            torch.cuda.set_device(args.gpu)\n",
        "            networks = [x.cuda(args.gpu) for x in networks]\n",
        "        else:\n",
        "            networks = [torch.nn.DataParallel(x).cuda() for x in networks]\n",
        "\n",
        "        discriminator, generator, = networks\n",
        "\n",
        "        # Update the networks at finest scale\n",
        "        if args.distributed:\n",
        "            for net_idx in range(generator.module.current_scale):\n",
        "                for param in generator.module.sub_generators[net_idx].parameters():\n",
        "                    param.requires_grad = False\n",
        "                for param in discriminator.module.sub_discriminators[net_idx].parameters():\n",
        "                    param.requires_grad = False\n",
        "\n",
        "            d_opt = torch.optim.Adam(discriminator.module.sub_discriminators[discriminator.current_scale].parameters(),\n",
        "                                     5e-4, (0.5, 0.999))\n",
        "            g_opt = torch.optim.Adam(generator.module.sub_generators[generator.current_scale].parameters(),\n",
        "                                     5e-4, (0.5, 0.999))\n",
        "        else:\n",
        "            for net_idx in range(generator.current_scale):\n",
        "                for param in generator.sub_generators[net_idx].parameters():\n",
        "                    param.requires_grad = False\n",
        "                for param in discriminator.sub_discriminators[net_idx].parameters():\n",
        "                    param.requires_grad = False\n",
        "\n",
        "            d_opt = torch.optim.Adam(discriminator.sub_discriminators[discriminator.current_scale].parameters(),\n",
        "                                     5e-4, (0.5, 0.999))\n",
        "            g_opt = torch.optim.Adam(generator.sub_generators[generator.current_scale].parameters(),\n",
        "                                     5e-4, (0.5, 0.999))\n",
        "\n",
        "        ##############\n",
        "        # Save model #\n",
        "        ##############\n",
        "        if not args.multiprocessing_distributed or (args.multiprocessing_distributed and args.rank % ngpus_per_node == 0):\n",
        "            if stage == 0:\n",
        "                check_list = open(os.path.join(args.log_dir, \"checkpoint.txt\"), \"a+\")\n",
        "            save_checkpoint({\n",
        "                'stage': stage + 1,\n",
        "                'D_state_dict': discriminator.state_dict(),\n",
        "                'G_state_dict': generator.state_dict(),\n",
        "                'd_optimizer': d_opt.state_dict(),\n",
        "                'g_optimizer': g_opt.state_dict(),\n",
        "                'img_to_use': args.img_to_use\n",
        "            }, check_list, args.log_dir, stage + 1)\n",
        "            if stage == args.num_scale:\n",
        "                check_list.close()\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVNqY1SMg17e",
        "colab_type": "code",
        "outputId": "728c6da8-0901-4915-fd3b-0a6be25d5460",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 726
        }
      },
      "source": [
        "\n",
        "\n",
        "args = Arguments()\n",
        "args.gpu=0\n",
        "args.distributed = False\n",
        "args.load_model = None\n",
        "args.batch_size = 2\n",
        "args.img_size_max =  224\n",
        "args.img_size_min = 28\n",
        "args.heads = 8\n",
        "args.d_model = 3\n",
        "args.dim1 = 224\n",
        "args.dim2 = 224\n",
        "args.dropout = 0.4 \n",
        "args.dataset = \"train\"\n",
        "\n",
        "if args.gpu is not None:\n",
        "   \n",
        "    warnings.warn('You have chosen a specific GPU. This will completely '\n",
        "                  'disable data parallelism.')\n",
        "\"\"\"\n",
        "args.distributed = args.world_size > 1 or args.multiprocessing_distributed\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "ngpus_per_node = torch.cuda.device_count()\n",
        "args.load_model = None\n",
        "if args.load_model is None:\n",
        "    args.model_name = '{}_{}'.format(args.model_name, datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
        "else:\n",
        "    args.model_name = args.load_model\n",
        "\n",
        "makedirs('./logs')\n",
        "makedirs('./results')\n",
        "\n",
        "args.log_dir = os.path.join('./logs', args.model_name)\n",
        "args.res_dir = os.path.join('./results', args.model_name)\n",
        "\n",
        "makedirs(args.log_dir)\n",
        "makedirs(os.path.join(args.log_dir, 'codes'))\n",
        "makedirs(os.path.join(args.log_dir, 'codes', 'models'))\n",
        "makedirs(args.res_dir)\n",
        "\n",
        "if args.load_model is None:\n",
        "    pyfiles = glob(\"./*.py\")\n",
        "    modelfiles = glob('./models/*.py')\n",
        "    for py in pyfiles:\n",
        "        copyfile(py, os.path.join(args.log_dir, 'codes') + \"/\" + py)\n",
        "    for py in modelfiles:\n",
        "        copyfile(py, os.path.join(args.log_dir, 'codes', py[2:]))\n",
        "\n",
        "formatted_print('Total Number of GPUs:', ngpus_per_node)\n",
        "formatted_print('Total Number of Workers:', args.workers)\n",
        "formatted_print('Batch Size:', args.batch_size)\n",
        "formatted_print('Max image Size:', args.img_size_max)\n",
        "formatted_print('Min image Size:', args.img_size_min)\n",
        "formatted_print('Log DIR:', args.log_dir)\n",
        "formatted_print('Result DIR:', args.res_dir)\n",
        "formatted_print('GAN TYPE:', args.gantype)\n",
        "\n",
        "if args.multiprocessing_distributed:\n",
        "    args.world_size = ngpus_per_node * args.world_size\n",
        "    mp.spawn(main_worker, nprocs=ngpus_per_node, args=(ngpus_per_node, args))\n",
        "else:\n",
        "    main_worker(args.gpu, ngpus_per_node, args)\n",
        "\n"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Total Number of GPUs:                   1                                       \n",
            "Total Number of Workers:                8                                       \n",
            "Batch Size:                             2                                       \n",
            "Max image Size:                         224                                     \n",
            "Min image Size:                         28                                      \n",
            "Log DIR:                                ./logs/SinGan_2019-11-20_17-33-37       \n",
            "Result DIR:                             ./results/SinGan_2019-11-20_17-33-37    \n",
            "GAN TYPE:                               zerogp                                  \n",
            "Use GPU: 0 for training\n",
            "[28, 37, 49, 66, 88, 117, 157, 209]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/2000 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 3, 18, 18])\n",
            "torch.Size([2, 3, 28, 28])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:73: UserWarning: Using a target size (torch.Size([2, 3, 28, 28])) that is different to the input size (torch.Size([2, 3, 18, 18])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-e56ec8f1a615>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_worker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnprocs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mngpus_per_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngpus_per_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mmain_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngpus_per_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-71-980e39503a92>\u001b[0m in \u001b[0;36mmain_worker\u001b[0;34m(gpu, ngpus_per_node, args)\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0mtrain_sampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mtrainSinGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetworks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"d_opt\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0md_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"g_opt\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mg_opt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"z_rec\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mz_fix_list\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m        \u001b[0;31m# validateSinGAN(train_loader, networks, stage, args, {\"z_rec\": z_fix_list})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-62-205f721bbf34>\u001b[0m in \u001b[0;36mtrainSinGAN\u001b[0;34m(data_loader, networks, opts, stage, args, additional)\u001b[0m\n\u001b[1;32m     71\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_rec_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m           \u001b[0mg_rec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_rec_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m           \u001b[0;31m# calculate rmse for each scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m           \u001b[0mrmse_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2201\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mean'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2202\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2203\u001b[0;31m         \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2204\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2205\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 [0, 1, 2]])\n\u001b[1;32m     51\u001b[0m     \"\"\"\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (18) must match the size of tensor b (28) at non-singleton dimension 3"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7uxyj2sg1z8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9C5Zl0yg1o7",
        "colab_type": "code",
        "outputId": "ba08bd2f-5a5a-4ed5-f7ce-6d07d5f4415b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        }
      },
      "source": [
        "len(1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-1cf91bb60cc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'int' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWKnxJT2Tnmz",
        "colab_type": "code",
        "outputId": "c5c315e9-9fd1-4c4c-fe0a-9d13a61ad37b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "w = torch.empty(500,300)\n",
        "a=  torch.empty(500,300)\n",
        "a= nn.init.kaiming_uniform_(a, mode='fan_in')\n",
        "\n",
        "\n",
        "\n",
        "nn.init.kaiming_uniform_(w, mode='fan_in')\n",
        "(a*w).argmax()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(12876)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJd7tM_bTnm0",
        "colab_type": "code",
        "outputId": "062de05e-f4fc-4bca-d75c-8426e64608fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "w = torch.rand(32,256,20,20)\n",
        "w.argmax()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2207442)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13vH6UQ-GaT1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9JyekUdVRW8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a=torch.chunk(w,8,dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJur-8yHVVk6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c=torch.reshape(w,(8,32,32,20,20))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yb2ul3serKvL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d=torch.stack(a,dim=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyB0dPfNtEN0",
        "colab_type": "code",
        "outputId": "f4e29fd9-5102-42bb-f0ab-83e5d8f85a14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.equal(c,d)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6gsdG7ouNP7",
        "colab_type": "code",
        "outputId": "5fb0720f-3472-4e58-d981-81a1224c402d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.all(torch.lt(torch.abs(torch.add(c, -d)), 1e-1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hejHXcsque3d",
        "colab_type": "code",
        "outputId": "16585917-d437-41c8-cc1b-417f4668b541",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "sz=4\n",
        "mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "print(mask)\n",
        "mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ True, False, False, False],\n",
            "        [ True,  True, False, False],\n",
            "        [ True,  True,  True, False],\n",
            "        [ True,  True,  True,  True]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgzbZ-hMwCHZ",
        "colab_type": "code",
        "outputId": "894a11d3-766c-48f1-b3fd-b1a1210b7345",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "mask"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., -inf, -inf, -inf],\n",
              "        [0., 0., -inf, -inf],\n",
              "        [0., 0., 0., -inf],\n",
              "        [0., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5DKNpWxwDGR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mask = (torch.triu(torch.ones(sz, sz)) == 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXrtPkahBpC3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mult_4d( batch, q_w, bs):\n",
        "  layers = batch.shape[1]\n",
        "  dim1 = batch.shape[2]\n",
        "  dim2 = batch.shape[3]\n",
        "  output = torch.bmm(batch.view(bs*layers,dim1,dim2), q_w.view(bs*layers,dim1,dim2))\n",
        "  return output.view(bs,layers,dim1,dim2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LKT4Uxxi1Ca",
        "colab_type": "code",
        "outputId": "2d3678cf-cb99-41c0-dece-d5b748e97095",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "a=torch.tensor([[1,2],[3,4]])\n",
        "b=torch.tensor([[1,2],[3,4]])\n",
        "b@a\n",
        "#torch.bmm(a.unsqueeze(0),b.unsqueeze(0))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 7, 10],\n",
              "        [15, 22]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1X52HvABqQr",
        "colab_type": "code",
        "outputId": "d855b7bf-fc8f-4696-b0d1-90e5b972b169",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "w = torch.rand(4,2,2,2)\n",
        "a = torch.rand(4,2,2,2)\n",
        "o=mult_4d(w,a.transpose(-2,-1),4)\n",
        "#torch.bmm(w,a).shape\n",
        "l=nn.Dropout()\n",
        "l(o).shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 2, 2, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJaUxw9xUnzs",
        "colab_type": "code",
        "outputId": "3936a479-eb33-4880-c0ca-95ca559090ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "def generate_square_subsequent_mask(sz):\n",
        "        r\"\"\"Generate a square mask for the sequence. The masked positions are filled with float('-inf').\n",
        "            Unmasked positions are filled with float(0.0).\n",
        "        \"\"\"\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "mask= generate_square_subsequent_mask(4)\n",
        "mask"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., -inf, -inf, -inf],\n",
              "        [0., 0., -inf, -inf],\n",
              "        [0., 0., 0., -inf],\n",
              "        [0., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATIum5xpVa22",
        "colab_type": "code",
        "outputId": "df4b70ac-cc97-427d-bdca-a113b14f595a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        }
      },
      "source": [
        "mask.unsqueeze(0).unsqueeze(0)+a"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-08fed53c7ecf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (2) at non-singleton dimension 3"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCso7oABVzfe",
        "colab_type": "code",
        "outputId": "fba15462-f265-46fc-89e9-b98668cc655c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "mask.unsqueeze(0).unsqueeze(0).shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 4, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9Hl6lEHWcZ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l=nn.Softmax2d()#.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_UVlYWgXTwU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c= l(mult_4d(w,a,w.shape[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYV6hRkGYmoy",
        "colab_type": "code",
        "outputId": "f56e06e6-8800-457a-bbaf-317369b7171a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "c.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 2, 2, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixUho7w5a3zX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l=nn.Softmax2d()#.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eh4Dud3a6bV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c=l(mult_4d(w,a,w.shape[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}