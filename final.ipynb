{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "# from torch.nn import init\n",
    "import functools\n",
    "import torchvision.models as models\n",
    "\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "\n",
    "from torchvision import models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms, utils\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Suppose you are trying to load pre-trained resnet model in directory- models\\resnet\n",
    "\n",
    "os.environ['TORCH_HOME'] = 'D:\\dev\\Pytorch_Models\\models\\\\resnet' #setting the environment variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import namedtuple\n",
    "import zipfile\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class CityscapeDataset(Dataset):\n",
    "    \"\"\"`Cityscapes <http://www.cityscapes-dataset.com/>`_ Dataset.\n",
    "\n",
    "    Args:\n",
    "        root (string): Root directory of dataset where directory ``leftImg8bit``\n",
    "            \n",
    "        split (string, optional): The image split to use, ``train``, ``train_extra`` or ``val``\n",
    "       \n",
    "        transform (callable, optional): A function/transform that takes in a PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.RandomCrop``  \n",
    "        transforms (callable, optional): A function/transform that takes input sample and its target as entry\n",
    "            and returns a transformed version.\n",
    "\n",
    "    Examples:\n",
    "\n",
    "      \n",
    "    \"\"\"\n",
    "\n",
    "    # Based on https://github.com/mcordts/cityscapesScripts\n",
    "  \n",
    "\n",
    "    def __init__(self, root=None, split='train_extra', transforms=None):\n",
    "        if root is not None:\n",
    "          self.images_dir = os.path.join(self.root, 'leftImg8bit', split)\n",
    "        else:\n",
    "          self.images_dir = os.path.join('leftImg8bit', split)\n",
    "        self.split = split\n",
    "        self.images = []\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "  \n",
    "        #valid_modes = (\"train\", \"train_extra\", \"val\")\n",
    "        \n",
    "        for city in os.listdir(self.images_dir):\n",
    "            img_dir = os.path.join(self.images_dir, city)\n",
    "            images_city=[]\n",
    "            for file_name in os.listdir(img_dir):\n",
    "                \n",
    "                images_city.append(os.path.join(img_dir, file_name))\n",
    "            # re.split(\\d+,input) splits by integer value\n",
    "            images_city.sort( key= lambda text: int(re.split('(\\d+)',text)[3]+re.split('(\\d+)',text) [5] ))\n",
    "            self.images+=images_city\n",
    "        \n",
    "    def print_dir(self):\n",
    "      print(self.images)\n",
    "\n",
    "    def im_path(self):\n",
    "      return self.images            \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "             image \n",
    "        \"\"\"\n",
    "\n",
    "        image = Image.open(self.images[index]).convert('RGB')\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        return image\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import namedtuple\n",
    "import zipfile\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class CityscapeDataset(Dataset):\n",
    "    \"\"\"`Cityscapes <http://www.cityscapes-dataset.com/>`_ Dataset.\n",
    "\n",
    "    Args:\n",
    "        root (string): Root directory of dataset where directory ``leftImg8bit``\n",
    "            \n",
    "        split (string, optional): The image split to use, ``train``, ``train_extra`` or ``val``\n",
    "       \n",
    "        transform (callable, optional): A function/transform that takes in a PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.RandomCrop``  \n",
    "        transforms (callable, optional): A function/transform that takes input sample and its target as entry\n",
    "            and returns a transformed version.\n",
    "\n",
    "    Examples:\n",
    "\n",
    "      \n",
    "    \"\"\"\n",
    "\n",
    "    # Based on https://github.com/mcordts/cityscapesScripts\n",
    "  \n",
    "\n",
    "    def __init__(self, root=None, split='train_extra', transforms=None):\n",
    "        if root is not None:\n",
    "          self.images_dir = os.path.join(self.root, 'leftImg8bit', split)\n",
    "        else:\n",
    "          self.images_dir = os.path.join('leftImg8bit', split)\n",
    "        self.split = split\n",
    "        self.images = []\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "  \n",
    "        #valid_modes = (\"train\", \"train_extra\", \"val\")\n",
    "        \n",
    "        for city in os.listdir(self.images_dir):\n",
    "            img_dir = os.path.join(self.images_dir, city)\n",
    "            images_city=[]\n",
    "            for file_name in os.listdir(img_dir):\n",
    "                \n",
    "                images_city.append(os.path.join(img_dir, file_name))\n",
    "            # re.split(\\d+,input) splits by integer value\n",
    "            images_city.sort( key= lambda text: int(re.split('(\\d+)',text)[3]+re.split('(\\d+)',text) [5] ))\n",
    "            self.images+=images_city\n",
    "        \n",
    "    def print_dir(self):\n",
    "      print(self.images)\n",
    "\n",
    "    def im_path(self):\n",
    "      return self.images            \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "             image \n",
    "        \"\"\"\n",
    "\n",
    "        image = Image.open(self.images[index]).convert('RGB')\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        return image\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = transforms.Resize((224, 224))\n",
    "to_tensor = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not adding normalization as image looses textures and clrs for recostrucyion\n",
    "\n",
    "composed_transforms=transforms.Compose([scaler,to_tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dataset = CityscapeDataset(\n",
    "                                           transforms=composed_transforms\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(transformed_dataset, batch_size=4,\n",
    "                        sampler=torch.utils.data.SequentialSampler(transformed_dataset) , num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50Bottom(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(ResNet50Bottom, self).__init__()\n",
    "        self.features = nn.Sequential(*list(original_model.children())[:-5])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res50_model = models.resnet50(pretrained=True)\n",
    "res50_conv2 = ResNet50Bottom(res50_model).cuda()\n",
    "\n",
    "outputs = res50_conv2(x.to('cuda'))\n",
    "outputs.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // heads\n",
    "        self.h = heads\n",
    "        \n",
    "        w = torch.empty(bs,\n",
    "        \n",
    "        \n",
    "        a,b, requires_grad=True)\n",
    "\n",
    "        self.q_3d=nn.init.kaiming_normal_(w, mode='fan_in')\n",
    "        self.v_3d=nn.init.kaiming_normal_(w, mode='fan_in')\n",
    "        self.k_3d=nn.init.kaiming_normal_(w, mode='fan_in')\n",
    "        \n",
    "        #self.q_linear = nn.Linear(d_model, d_model)\n",
    "        #self.v_linear = nn.Linear(d_model, d_model)\n",
    "        #self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \n",
    "        bs = q.size(0)\n",
    "        \n",
    "        # perform linear operation and split into h heads\n",
    "        \n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
    "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
    "        \n",
    "        # transpose to get dimensions bs * h * sl * d_model\n",
    "       \n",
    "        k = k.transpose(1,2)\n",
    "        q = q.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "# calculate attention using function we will define next\n",
    "        scores = attention(q, k, v, self.d_k, mask, self.dropout)\n",
    "        \n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = scores.transpose(1,2).contiguous()\\\n",
    "        .view(bs, -1, self.d_model)\n",
    "        \n",
    "        output = self.out(concat)\n",
    "    \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}