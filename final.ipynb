{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3,
    "colab": {
      "name": "final.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nilanshrajput/Video_Generation_Transformer/blob/master/final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RJz4WUwTnmc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "from collections import OrderedDict\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "import functools\n",
        "import torchvision.models as models\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "\n",
        "from torchvision import models\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torch.autograd import Variable\n",
        "from torchvision import transforms, utils\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from skimage import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaBLoPcRTnmg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "# Suppose you are trying to load pre-trained resnet model in directory- models\\resnet\n",
        "\n",
        "os.environ['TORCH_HOME'] = 'D:\\dev\\Pytorch_Models\\models\\\\resnet' #setting the environment variable\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLdnAS6ETnmi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riBpMJFiTnmj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import json\n",
        "import os\n",
        "from collections import namedtuple\n",
        "import zipfile\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "class CityscapeDataset(Dataset):\n",
        "    \"\"\"`Cityscapes <http://www.cityscapes-dataset.com/>`_ Dataset.\n",
        "\n",
        "    Args:\n",
        "        root (string): Root directory of dataset where directory ``leftImg8bit``\n",
        "            \n",
        "        split (string, optional): The image split to use, ``train``, ``train_extra`` or ``val``\n",
        "       \n",
        "        transform (callable, optional): A function/transform that takes in a PIL image\n",
        "            and returns a transformed version. E.g, ``transforms.RandomCrop``  \n",
        "        transforms (callable, optional): A function/transform that takes input sample and its target as entry\n",
        "            and returns a transformed version.\n",
        "\n",
        "    Examples:\n",
        "\n",
        "      \n",
        "    \"\"\"\n",
        "\n",
        "    # Based on https://github.com/mcordts/cityscapesScripts\n",
        "  \n",
        "\n",
        "    def __init__(self, root=None, split='train_extra', transforms=None):\n",
        "        if root is not None:\n",
        "          self.images_dir = os.path.join(self.root, 'leftImg8bit', split)\n",
        "        else:\n",
        "          self.images_dir = os.path.join('leftImg8bit', split)\n",
        "        self.split = split\n",
        "        self.images = []\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "  \n",
        "        #valid_modes = (\"train\", \"train_extra\", \"val\")\n",
        "        \n",
        "        for city in os.listdir(self.images_dir):\n",
        "            img_dir = os.path.join(self.images_dir, city)\n",
        "            images_city=[]\n",
        "            for file_name in os.listdir(img_dir):\n",
        "                \n",
        "                images_city.append(os.path.join(img_dir, file_name))\n",
        "            # re.split(\\d+,input) splits by integer value\n",
        "            images_city.sort( key= lambda text: int(re.split('(\\d+)',text)[3]+re.split('(\\d+)',text) [5] ))\n",
        "            self.images+=images_city\n",
        "        \n",
        "    def print_dir(self):\n",
        "      print(self.images)\n",
        "\n",
        "    def im_path(self):\n",
        "      return self.images            \n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "        Returns:\n",
        "             image \n",
        "        \"\"\"\n",
        "\n",
        "        image = Image.open(self.images[index]).convert('RGB')\n",
        "        if self.transforms is not None:\n",
        "            image = self.transforms(image)\n",
        "\n",
        "        return image\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6K9KmFu2Tnml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import os\n",
        "from collections import namedtuple\n",
        "import zipfile\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "class CityscapeDataset(Dataset):\n",
        "    \"\"\"`Cityscapes <http://www.cityscapes-dataset.com/>`_ Dataset.\n",
        "\n",
        "    Args:\n",
        "        root (string): Root directory of dataset where directory ``leftImg8bit``\n",
        "            \n",
        "        split (string, optional): The image split to use, ``train``, ``train_extra`` or ``val``\n",
        "       \n",
        "        transform (callable, optional): A function/transform that takes in a PIL image\n",
        "            and returns a transformed version. E.g, ``transforms.RandomCrop``  \n",
        "        transforms (callable, optional): A function/transform that takes input sample and its target as entry\n",
        "            and returns a transformed version.\n",
        "\n",
        "    Examples:\n",
        "\n",
        "      \n",
        "    \"\"\"\n",
        "\n",
        "    # Based on https://github.com/mcordts/cityscapesScripts\n",
        "  \n",
        "\n",
        "    def __init__(self, root=None, split='train_extra', transforms=None):\n",
        "        if root is not None:\n",
        "          self.images_dir = os.path.join(self.root, 'leftImg8bit', split)\n",
        "        else:\n",
        "          self.images_dir = os.path.join('leftImg8bit', split)\n",
        "        self.split = split\n",
        "        self.images = []\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "  \n",
        "        #valid_modes = (\"train\", \"train_extra\", \"val\")\n",
        "        \n",
        "        for city in os.listdir(self.images_dir):\n",
        "            img_dir = os.path.join(self.images_dir, city)\n",
        "            images_city=[]\n",
        "            for file_name in os.listdir(img_dir):\n",
        "                \n",
        "                images_city.append(os.path.join(img_dir, file_name))\n",
        "            # re.split(\\d+,input) splits by integer value\n",
        "            images_city.sort( key= lambda text: int(re.split('(\\d+)',text)[3]+re.split('(\\d+)',text) [5] ))\n",
        "            self.images+=images_city\n",
        "        \n",
        "    def print_dir(self):\n",
        "      print(self.images)\n",
        "\n",
        "    def im_path(self):\n",
        "      return self.images            \n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "        Returns:\n",
        "             image \n",
        "        \"\"\"\n",
        "\n",
        "        image = Image.open(self.images[index]).convert('RGB')\n",
        "        if self.transforms is not None:\n",
        "            image = self.transforms(image)\n",
        "\n",
        "        return image\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LdC3zJtTnmm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaler = transforms.Resize((224, 224))\n",
        "to_tensor = transforms.ToTensor()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpSu6DcVTnmo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#not adding normalization as image looses textures and clrs for recostruction\n",
        "\n",
        "composed_transforms=transforms.Compose([scaler,to_tensor])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "598CAaijTnmq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transformed_dataset = CityscapeDataset(\n",
        "                                           transforms=composed_transforms\n",
        "                                           )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RV0CGV5kTnmr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataloader = DataLoader(transformed_dataset, batch_size=4,\n",
        "                        sampler=torch.utils.data.SequentialSampler(transformed_dataset) , num_workers=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvSg9ccTTnmt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x=next(iter(dataloader))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iJugmqRTnmu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResNet50Bottom(nn.Module):\n",
        "    def __init__(self, original_model):\n",
        "        super(ResNet50Bottom, self).__init__()\n",
        "        self.features = nn.Sequential(*list(original_model.children())[:-5])\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5P4hTd5fTnmw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "res50_model = models.resnet50(pretrained=True)\n",
        "res50_conv2 = ResNet50Bottom(res50_model).cuda()\n",
        "\n",
        "outputs = res50_conv2(x.to('cuda'))\n",
        "outputs.data.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBMR6P1dDsQh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FeatureChanger(nn.Module):\n",
        "    def __init__(self,in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.kernel_size = 1\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.conv2d_0 = torch.nn.Conv2d(in_channels, out_channels*2, kernel_size, stride=1)\n",
        "        self.con2d_1 = torch.nn.Conv2d(out_channels*2, out_channels, kernel_size, stride=1)\n",
        "        \n",
        "    def forward(self, batch):\n",
        "        x = self.conv2d_0(batch)\n",
        "        x = self.con2d_1(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4cE65evRDKb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MaskedSelfAttention(nn.Module):\n",
        "\n",
        "  def __init__(self,heads, d_model):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.heads = heads\n",
        "\n",
        "\n",
        "  def _generate_square_subsequent_mask(self, sz):\n",
        "    #sz= batch size\n",
        "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "  def mult_4d(self, batch, q_w, bs):\n",
        "    layers = batch.shape[1]\n",
        "    dim1 = batch.shape[2]\n",
        "    dim2 = batch.shape[3]\n",
        "    output = torch.bmm(batch.view(bs*layers,dim1,dim2), q_w.view(bs*layers,dim1,dim2))\n",
        "    return output.view(bs,layers,dim1,dim2)\n",
        "\n",
        "  def forward(self):\n",
        "\n",
        "    #score\n",
        "    attn_output_weights = mult_4d(q, k)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-iPL_1-Tnmx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, heads, d_model, dropout = 0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.d_k = d_model // heads\n",
        "        self.h = heads\n",
        "        \n",
        "        self.q_3d = torch.nn.Parameter(torch.empty(bs,nu_feat,dim1,dim2,requires_grad=True))\n",
        "        self.k_3d = torch.nn.Parameter(torch.empty(bs,nu_feat,dim1,dim2,requires_grad=True))\n",
        "        self.v_3d = torch.nn.Parameter(torch.empty(bs,nu_feat,dim1,dim2,requires_grad=True))\n",
        "   \n",
        "\n",
        "        nn.init.kaiming_normal_(q_3d, mode='fan_in')\n",
        "        nn.init.kaiming_normal_(k_3d, mode='fan_in')\n",
        "        nn.init.kaiming_normal_(v_3d, mode='fan_in')\n",
        "\n",
        "        self.attention_layer= MaskedSelfAttention()\n",
        "        \n",
        "        \n",
        "        #self.q_linear = nn.Linear(d_model, d_model)\n",
        "        #self.v_linear = nn.Linear(d_model, d_model)\n",
        "        #self.k_linear = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.out = nn.Linear(d_model, d_model)\n",
        "    \n",
        "    def mult_4d(self, batch, q_w, bs):\n",
        "      layers = batch.shape[1]\n",
        "      dim1 = batch.shape[2]\n",
        "      dim2 = batch.shape[3]\n",
        "      output = torch.bmm(batch.view(bs*layers,dim1,dim2), q_w.view(bs*layers,dim1,dim2))\n",
        "      return output.view(bs,layers,dim1,dim2)\n",
        "\n",
        "    def forward(self, batch, mask=None):\n",
        "        \n",
        "        bs = q.size(0)\n",
        "        features_changer = FeatureChanger(batch.shape[1],batch.shape[1]/self.h*2)\n",
        "        batch = feature_changer(batch)\n",
        "\n",
        "        q = mult_4d(batch,self.q_3d,bs)\n",
        "        k = mult_4d(batch,self.k_3d,bs)\n",
        "        v = mult_4d(batch,self.v_3d,bs)\n",
        "        #divide feature space in no of heads each head works on certain features only\n",
        "        #returns tuple of size=heads\n",
        "\n",
        "        batch_chunks= torch.chunk(batch, heads, dim=1)\n",
        "        #converting tuple to tensor\n",
        "        batch_chunks = torch.stack(batch_chunks, dim =0)\n",
        "\n",
        "        attention_out= attention_layer(batch_chunks, q, k, v)\n",
        "        \n",
        "        \n",
        "        # perform linear operation and split into h heads\n",
        "        \n",
        "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
        "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
        "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
        "        \n",
        "        # transpose to get dimensions bs * h * sl * d_model\n",
        "       \n",
        "        k = k.transpose(1,2)\n",
        "        q = q.transpose(1,2)\n",
        "        v = v.transpose(1,2)\n",
        "# calculate attention using function we will define next\n",
        "        scores = attention(q, k, v, self.d_k, mask, self.dropout)\n",
        "        \n",
        "        # concatenate heads and put through final linear layer\n",
        "        concat = scores.transpose(1,2).contiguous()\\\n",
        "        .view(bs, -1, self.d_model)\n",
        "        \n",
        "        output = self.out(concat)\n",
        "    \n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWKnxJT2Tnmz",
        "colab_type": "code",
        "outputId": "9cedcbec-71c5-461d-8f5a-99c24249251e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "w = torch.empty(500,300)\n",
        "a=  torch.empty(500,300)\n",
        "a= nn.init.kaiming_uniform_(a, mode='fan_in')\n",
        "\n",
        "\n",
        "\n",
        "nn.init.kaiming_uniform_(w, mode='fan_in')\n",
        "(a*w).argmax()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(29802)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJd7tM_bTnm0",
        "colab_type": "code",
        "outputId": "062de05e-f4fc-4bca-d75c-8426e64608fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "w = torch.rand(32,256,20,20)\n",
        "w.argmax()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2207442)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13vH6UQ-GaT1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9JyekUdVRW8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a=torch.chunk(w,8,dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJur-8yHVVk6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c=torch.reshape(w,(8,32,32,20,20))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yb2ul3serKvL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d=torch.stack(a,dim=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyB0dPfNtEN0",
        "colab_type": "code",
        "outputId": "f4e29fd9-5102-42bb-f0ab-83e5d8f85a14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.equal(c,d)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6gsdG7ouNP7",
        "colab_type": "code",
        "outputId": "5fb0720f-3472-4e58-d981-81a1224c402d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.all(torch.lt(torch.abs(torch.add(c, -d)), 1e-1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hejHXcsque3d",
        "colab_type": "code",
        "outputId": "16585917-d437-41c8-cc1b-417f4668b541",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "sz=4\n",
        "mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "print(mask)\n",
        "mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ True, False, False, False],\n",
            "        [ True,  True, False, False],\n",
            "        [ True,  True,  True, False],\n",
            "        [ True,  True,  True,  True]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgzbZ-hMwCHZ",
        "colab_type": "code",
        "outputId": "894a11d3-766c-48f1-b3fd-b1a1210b7345",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "mask"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., -inf, -inf, -inf],\n",
              "        [0., 0., -inf, -inf],\n",
              "        [0., 0., 0., -inf],\n",
              "        [0., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5DKNpWxwDGR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mask = (torch.triu(torch.ones(sz, sz)) == 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXrtPkahBpC3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mult_4d( batch, q_w, bs):\n",
        "  layers = batch.shape[1]\n",
        "  dim1 = batch.shape[2]\n",
        "  dim2 = batch.shape[3]\n",
        "  output = torch.bmm(batch.view(bs*layers,dim1,dim2), q_w.view(bs*layers,dim1,dim2))\n",
        "  return output.view(bs,layers,dim1,dim2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1X52HvABqQr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w = torch.rand(4,2,2,2)\n",
        "a = torch.rand(4,2,2,2)\n",
        "#torch.bmm(w,a).shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJaUxw9xUnzs",
        "colab_type": "code",
        "outputId": "3936a479-eb33-4880-c0ca-95ca559090ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "def generate_square_subsequent_mask(sz):\n",
        "        r\"\"\"Generate a square mask for the sequence. The masked positions are filled with float('-inf').\n",
        "            Unmasked positions are filled with float(0.0).\n",
        "        \"\"\"\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "mask= generate_square_subsequent_mask(4)\n",
        "mask"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., -inf, -inf, -inf],\n",
              "        [0., 0., -inf, -inf],\n",
              "        [0., 0., 0., -inf],\n",
              "        [0., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATIum5xpVa22",
        "colab_type": "code",
        "outputId": "df4b70ac-cc97-427d-bdca-a113b14f595a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        }
      },
      "source": [
        "mask.unsqueeze(0).unsqueeze(0)+a"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-08fed53c7ecf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (2) at non-singleton dimension 3"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCso7oABVzfe",
        "colab_type": "code",
        "outputId": "fba15462-f265-46fc-89e9-b98668cc655c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "mask.unsqueeze(0).unsqueeze(0).shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 4, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9Hl6lEHWcZ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l=nn.Softmax2d()#.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_UVlYWgXTwU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c= l(mult_4d(w,a,w.shape[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYV6hRkGYmoy",
        "colab_type": "code",
        "outputId": "f56e06e6-8800-457a-bbaf-317369b7171a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "c.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 2, 2, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixUho7w5a3zX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l=nn.Softmax2d()#.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eh4Dud3a6bV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c=l(mult_4d(w,a,w.shape[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}