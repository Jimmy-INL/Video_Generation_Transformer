{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "# from torch.nn import init\n",
    "import functools\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms, utils\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Suppose you are trying to load pre-trained resnet model in directory- models\\resnet\n",
    "\n",
    "os.environ['TORCH_HOME'] = 'D:\\dev\\Pytorch_Models\\models\\\\resnet' #setting the environment variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import namedtuple\n",
    "import zipfile\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class CityscapeDataset(Dataset):\n",
    "    \"\"\"`Cityscapes <http://www.cityscapes-dataset.com/>`_ Dataset.\n",
    "\n",
    "    Args:\n",
    "        root (string): Root directory of dataset where directory ``leftImg8bit``\n",
    "            \n",
    "        split (string, optional): The image split to use, ``train``, ``train_extra`` or ``val``\n",
    "       \n",
    "        transform (callable, optional): A function/transform that takes in a PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.RandomCrop``  \n",
    "        transforms (callable, optional): A function/transform that takes input sample and its target as entry\n",
    "            and returns a transformed version.\n",
    "\n",
    "    Examples:\n",
    "\n",
    "      \n",
    "    \"\"\"\n",
    "\n",
    "    # Based on https://github.com/mcordts/cityscapesScripts\n",
    "  \n",
    "\n",
    "    def __init__(self, root=None, split='train_extra', transforms=None):\n",
    "        if root is not None:\n",
    "          self.images_dir = os.path.join(self.root, 'leftImg8bit', split)\n",
    "        else:\n",
    "          self.images_dir = os.path.join('leftImg8bit', split)\n",
    "        self.split = split\n",
    "        self.images = []\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "  \n",
    "        #valid_modes = (\"train\", \"train_extra\", \"val\")\n",
    "        \n",
    "        for city in os.listdir(self.images_dir):\n",
    "            img_dir = os.path.join(self.images_dir, city)\n",
    "            images_city=[]\n",
    "            for file_name in os.listdir(img_dir):\n",
    "                \n",
    "                images_city.append(os.path.join(img_dir, file_name))\n",
    "            # re.split(\\d+,input) splits by integer value\n",
    "            images_city.sort( key= lambda text: int(re.split('(\\d+)',text)[3]+re.split('(\\d+)',text) [5] ))\n",
    "            self.images+=images_city\n",
    "        \n",
    "    def print_dir(self):\n",
    "      print(self.images)\n",
    "\n",
    "    def im_path(self):\n",
    "      return self.images            \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "             image \n",
    "        \"\"\"\n",
    "\n",
    "        image = Image.open(self.images[index]).convert('RGB')\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        return image\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = transforms.Resize((224, 224))\n",
    "to_tensor = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not adding normalization as image looses textures and clrs for recostrucyion\n",
    "\n",
    "composed_transforms=transforms.Compose([scaler,to_tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'images_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-c120d0f4a250>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m transformed_dataset = CityscapeDataset(\n\u001b[1;32m----> 2\u001b[1;33m                                            \u001b[0mtransforms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcomposed_transforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m                                            )\n",
      "\u001b[1;32m<ipython-input-65-a5fb526db100>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, split, transforms)\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;31m#valid_modes = (\"train\", \"train_extra\", \"val\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mcity\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m             \u001b[0mimg_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[0mimages_city\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'images_dir' is not defined"
     ]
    }
   ],
   "source": [
    "transformed_dataset = CityscapeDataset(\n",
    "                                           transforms=composed_transforms\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transformed_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-d5edcec8d290>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m dataloader = DataLoader(transformed_dataset, batch_size=4,\n\u001b[0m\u001b[0;32m      2\u001b[0m                         shuffle=True, num_workers=4)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'transformed_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(transformed_dataset, batch_size=4,\n",
    "                        sampler=torch.utils.data.SequentialSampler(transformed_dataset) , num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images=transformed_dataset.im_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, batch in enumerate(dataloader):\n",
    "        print(i, batch[1].shape)\n",
    "        w=10\n",
    "        h=10\n",
    "        fig=plt.figure(figsize=(8, 8))\n",
    "        columns = 1\n",
    "        rows = 4\n",
    "        for i,img in enumerate(batch):\n",
    "            img= (transforms.ToPILImage()(img))\n",
    "            img=np.clip(img,0,255)\n",
    "            #img=np.array(img)\n",
    "            #print(img)\n",
    "            fig.add_subplot(rows, columns, i+1)\n",
    "            plt.imshow(img)\n",
    "            \n",
    "        plt.show()\n",
    "\n",
    "        break"
   ]
  }
 ]
}